{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b74657",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc0a9e4",
   "metadata": {},
   "source": [
    "This work aims to provide a basic understanding of Conditional random fields (CRF), Viterbi algorithm and\n",
    "Markov chains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba212e1b",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84047931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5bea9",
   "metadata": {},
   "source": [
    "Task 1. Creating a CRF model to predict a sequence of 15 dice rolls (fair cube, biased cube).\n",
    "<br>Probabilities:\n",
    "* P (the first cube in the sequence is \"fair\") = 0.5\n",
    "* P (current \"fair\" cubes | previous \"fair\" cubes) = 0.8\n",
    "* P (current \"biased\" cubes | previous \"biased\" cubes) = 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51df7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dice, log_likelihood):\n",
    "        super(CRF, self).__init__()\n",
    "        self.n_states = n_dice\n",
    "        self.transition = torch.nn.init.normal(nn.Parameter(torch.randn(n_dice, n_dice + 1)), -1, 0.1)\n",
    "        self.loglikelihood = log_likelihood\n",
    "    \n",
    "\n",
    "    def to_scalar(self, var):\n",
    "        return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "    def argmax(self, vec):\n",
    "        _, idx = torch.max(vec, 1)\n",
    "        return self.to_scalar(idx)\n",
    "        \n",
    "    # numerically stable log sum exp\n",
    "    # source: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "    def log_sum_exp(self, vec):\n",
    "        max_score = vec[0, self.argmax(vec)]\n",
    "        max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "        return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "    \n",
    "    \n",
    "    def _data_to_likelihood(self, rolls):\n",
    "        \"\"\"Converts a numpy array of rolls (integers) to log-likelihood.\n",
    "        Input is one [1, n_rolls]\n",
    "        \"\"\"\n",
    "        return Variable(torch.FloatTensor(self.loglikelihood[rolls]), requires_grad=False)\n",
    "        \n",
    "    \n",
    "    def _compute_likelihood_numerator(self, loglikelihoods, states):\n",
    "        \"\"\"Computes numerator of likelihood function for a given sequence.\n",
    "        \n",
    "        We'll iterate over the sequence of states and compute the sum \n",
    "        of the relevant transition cost with the log likelihood of the observed\n",
    "        roll. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Matrix of n_obs x n_states. \n",
    "                            i,j entry is loglikelihood of observing roll i given state j\n",
    "            states: sequence of labels\n",
    "        Output:\n",
    "            score: torch Variable. Score of assignment. \n",
    "        \"\"\"\n",
    "        \n",
    "        prev_state = self.n_states\n",
    "        score = Variable(torch.Tensor([0]))\n",
    "        \n",
    "        for index, state in enumerate(states):\n",
    "            score += self.transition[state, prev_state] + loglikelihoods[index, state]\n",
    "            prev_state = state\n",
    "        return score\n",
    "    \n",
    "    def _compute_likelihood_denominator(self, loglikelihoods):\n",
    "        \"\"\"Implements the forward pass of the forward-backward algorithm.\n",
    "        \n",
    "        We loop over all possible states efficiently using the recursive\n",
    "        relationship: alpha_t(j) = \\sum_i alpha_{t-1}(i) * L(x_t | y_t) * C(y_t | y{t-1} = i)\n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_numerator.\n",
    "                            This algorithm efficiently loops over all possible state sequences\n",
    "                            so no other imput is needed.\n",
    "        Output:\n",
    "            torch Variable. \n",
    "        \"\"\"\n",
    "\n",
    "        # stores the current value of alpha at timestep t\n",
    "        prev_alpha = self.transition[:, self.n_states] + loglikelihoods[0].view(1, -1)\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            alpha_t = []\n",
    "\n",
    "            # loop over all possible states\n",
    "            for next_state in range(self.n_states):\n",
    "                \n",
    "                # compute all possible costs of transitioning to next_state\n",
    "                feature_function = self.transition[next_state, :self.n_states].view(1, -1) + \\\n",
    "                                   roll[next_state].view(1, -1).expand(1, self.n_states)\n",
    "                alpha_t_next_state = prev_alpha + feature_function\n",
    "                alpha_t.append(self.log_sum_exp(alpha_t_next_state))\n",
    "            \n",
    "            prev_alpha = torch.stack(alpha_t).view(1, -1)\n",
    "        \n",
    "        return self.log_sum_exp(prev_alpha)\n",
    "    \n",
    "    def _viterbi_algorithm(self, loglikelihoods):\n",
    "        \"\"\"Implements Viterbi algorithm for finding most likely sequence of labels.\n",
    "        \n",
    "        Very similar to _compute_likelihood_denominator but now we take the maximum\n",
    "        over the previous states as opposed to the sum. \n",
    "        Input:\n",
    "            loglikelihoods: torch Variable. Same input as _compute_likelihood_denominator.\n",
    "        Output:\n",
    "            tuple. First entry is the most likely sequence of labels. Second is\n",
    "                   the loglikelihood of this sequence. \n",
    "        \"\"\"\n",
    "\n",
    "        argmaxes = []\n",
    "        # prev_delta will store the current score of the sequence for each state\n",
    "        prev_delta = self.transition[:, self.n_states].contiguous().view(1, -1) + loglikelihoods[0].view(1, -1)\n",
    "\n",
    "        for roll in loglikelihoods[1:]:\n",
    "            local_argmaxes = []\n",
    "            next_delta = []\n",
    "            for next_state in range(self.n_states):\n",
    "                feature_function = self.transition[next_state,:self.n_states].view(1, -1) + roll.view(1, -1) + prev_delta\n",
    "                most_likely_state = self.argmax(feature_function)\n",
    "                score = feature_function[0][most_likely_state]\n",
    "                next_delta.append(score)\n",
    "                local_argmaxes.append(most_likely_state)\n",
    "            \n",
    "            prev_delta = torch.stack(next_delta).view(1, -1)\n",
    "            argmaxes.append(local_argmaxes)\n",
    "        \n",
    "        final_state = self.argmax(prev_delta)\n",
    "        final_score = prev_delta[0][final_state]\n",
    "        path_list = [final_state]\n",
    "\n",
    "        # backtrack through the argmaxes to find most likely state\n",
    "        for states in reversed(argmaxes):\n",
    "            final_state = states[final_state]\n",
    "            path_list.append(final_state)\n",
    "        \n",
    "        return np.array(path_list), final_score\n",
    "        \n",
    "    def neg_log_likelihood(self, rolls, states):\n",
    "        \"\"\"Compute neg log-likelihood for a given sequence.\n",
    "        \n",
    "        Input: \n",
    "            rolls: numpy array, dim [1, n_rolls]. Integer 0-5 showing value on dice.\n",
    "            states: numpy array, dim [1, n_rolls]. Integer 0, 1. 0 if dice is fair.\n",
    "        \"\"\"\n",
    "        \n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        states = torch.LongTensor(states)\n",
    "        sequence_loglik = self._compute_likelihood_numerator(loglikelihoods, states)\n",
    "        denominator = self._compute_likelihood_denominator(loglikelihoods)\n",
    "        return denominator - sequence_loglik\n",
    "               \n",
    "    \n",
    "    def forward(self, rolls):\n",
    "        loglikelihoods = self._data_to_likelihood(rolls)\n",
    "        return self._viterbi_algorithm(loglikelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b004e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crf_train_loop(model, rolls, targets, n_epochs, learning_rate=0.01):\n",
    "    # train of CRF with Adam optimizer\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        batch_loss = []\n",
    "        N = rolls.shape[0]\n",
    "        model.zero_grad()\n",
    "        \n",
    "        for index, (roll, labels) in enumerate(zip(rolls, targets)):\n",
    "            # forward pass\n",
    "            neg_log_likelihood = model.neg_log_likelihood(roll, labels)\n",
    "            batch_loss.append(neg_log_likelihood)\n",
    "            \n",
    "            if index % 50 == 0:\n",
    "                ll = torch.cat(batch_loss).mean()\n",
    "                ll.backward()\n",
    "                optimizer.step()\n",
    "                batch_loss = []\n",
    "        print(\"Epoch {}: loss is {:.4f}\".format(epoch, ll.data.numpy()))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9dc77",
   "metadata": {},
   "source": [
    "__Set input data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a05ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two dice one is fair, one is loaded\n",
    "fair_dice = np.array([1/6]*6)\n",
    "loaded_dice = np.array([0.04, 0.04, 0.04, 0.04, 0.04, 0.8])\n",
    "probabilities = {'fair': fair_dice,\n",
    "                'loaded': loaded_dice}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c8d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if dice is fair at time t, 0.6 chance we stay fair, 0.4 chance it is loaded at time 2\n",
    "transition_mat = {'fair': np.array([0.6, 0.4, 0.0]),\n",
    "                 'loaded': np.array([0.3, 0.7, 0.0]),\n",
    "                 'start': np.array([0.5, 0.5, 0.0])}\n",
    "states = ['fair', 'loaded', 'start']\n",
    "state2ix = {'fair': 0, 'loaded': 1, 'start': 2}\n",
    "\n",
    "log_likelihood = np.hstack([np.log(fair_dice).reshape(-1,1), np.log(loaded_dice).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a784104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data(n_timesteps):\n",
    "    data = np.zeros(n_timesteps)\n",
    "    prev_state = 'start'\n",
    "    state_list = np.zeros(n_timesteps)\n",
    "    for n in range(n_timesteps):\n",
    "        next_state = np.random.choice(states, p=transition_mat[prev_state])\n",
    "        state_list[n] = state2ix[next_state]\n",
    "        next_data = np.random.choice([0,1,2,3,4,5], p=probabilities[next_state])\n",
    "        data[n] = next_data\n",
    "        prev_state = next_state\n",
    "    return data, state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e3f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 15\n",
    "rolls = np.zeros((5000, n_obs)).astype(int)\n",
    "targets = np.zeros((5000, n_obs)).astype(int)\n",
    "\n",
    "for i in range(5000):\n",
    "    data, dices = simulate_data(n_obs)\n",
    "    rolls[i] = data.reshape(1, -1).astype(int)\n",
    "    targets[i] = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792d63ce",
   "metadata": {},
   "source": [
    "__Model training__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "633d4944",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nastia/Desktop/Intro_to_DS/venv/lib/python3.6/site-packages/ipykernel_launcher.py:6: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss is 6.5545\n",
      "Epoch 1: loss is 6.4096\n",
      "Epoch 2: loss is 6.3399\n",
      "Epoch 3: loss is 6.3023\n",
      "Epoch 4: loss is 6.2950\n",
      "Epoch 5: loss is 6.2904\n",
      "Epoch 6: loss is 6.2987\n",
      "Epoch 7: loss is 6.2963\n",
      "Epoch 8: loss is 6.3064\n",
      "Epoch 9: loss is 6.2998\n"
     ]
    }
   ],
   "source": [
    "model = CRF(2, log_likelihood)\n",
    "model = crf_train_loop(model, rolls, targets, 10, 0.001)\n",
    "torch.save(model.state_dict(), \"./checkpoint.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b17a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./checkpoint.hdf5\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce629fe",
   "metadata": {},
   "source": [
    "__Set test data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf4a329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dices = simulate_data(15)\n",
    "test_rolls = data.reshape(1, -1).astype(int)\n",
    "test_targets = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a4c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 0 5 5 5 0 4 5 2 3 5 3 5]\n",
      "[1 1 0 1 0 0 1 0 0 1 1 1 0 0 0]\n",
      "[0 0 0 0 0 1 1 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_rolls[0])\n",
    "print(model.forward(test_rolls[0])[0])\n",
    "print(test_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edee402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4652566  0.23195508 0.39975867]\n",
      " [0.27500758 0.50441575 0.38241985]]\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(list(model.parameters())[0].data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3323e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, dices = simulate_data(15)\n",
    "test_rolls = data.reshape(1, -1).astype(int)\n",
    "test_targets = dices.reshape(1, -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e16afd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 5 3 2 3 5 5 5 5 4 1 3 4 0 5]\n",
      "[1 1 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      "[0 1 0 0 0 1 1 0 1 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_rolls[0])\n",
    "print(model.forward(test_rolls[0])[0])\n",
    "print(test_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f55158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
